{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")  # создадим среду\n",
    "env.seed(17)  # зафиксируем сид для воспроизводимости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 4  # кол-во параметров состояния среды\n",
    "num_actions = 2  # кол-во возможных действий\n",
    "\n",
    "num_hidden = 64\n",
    "\n",
    "# создадим нейросеть при помощи Functional API\n",
    "# т.к. нам потребуется 2 выхода\n",
    "\n",
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "common_1 = layers.Dense(num_hidden, activation=\"relu\")(common)\n",
    "action = layers.Dense(num_actions, activation=\"softmax\")(common_1)\n",
    "critic = layers.Dense(1)(common_1)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, running reward: 8.15\n",
      "Episode: 20, running reward: 11.60\n",
      "Episode: 30, running reward: 12.10\n",
      "Episode: 40, running reward: 11.82\n",
      "Episode: 50, running reward: 12.53\n",
      "Episode: 60, running reward: 13.42\n",
      "Episode: 70, running reward: 12.62\n",
      "Episode: 80, running reward: 16.84\n",
      "Episode: 90, running reward: 22.16\n",
      "Episode: 100, running reward: 28.86\n",
      "Episode: 110, running reward: 34.70\n",
      "Episode: 120, running reward: 29.82\n",
      "Episode: 130, running reward: 26.64\n",
      "Episode: 140, running reward: 28.68\n",
      "Episode: 150, running reward: 31.56\n",
      "Episode: 160, running reward: 40.36\n",
      "Episode: 170, running reward: 57.56\n",
      "Episode: 180, running reward: 84.54\n",
      "Episode: 190, running reward: 90.89\n",
      "Episode: 200, running reward: 92.80\n",
      "Episode: 210, running reward: 132.84\n",
      "Episode: 220, running reward: 123.89\n",
      "Episode: 230, running reward: 115.69\n",
      "Episode: 240, running reward: 135.04\n",
      "Episode: 250, running reward: 156.74\n",
      "Episode: 260, running reward: 167.78\n",
      "Episode: 270, running reward: 163.31\n",
      "Episode: 280, running reward: 166.25\n",
      "Episode: 290, running reward: 175.05\n",
      "Episode: 300, running reward: 150.15\n",
      "Episode: 310, running reward: 149.17\n",
      "Episode: 320, running reward: 168.08\n",
      "Episode: 330, running reward: 180.60\n",
      "Episode: 340, running reward: 156.15\n",
      "Episode: 350, running reward: 173.45\n",
      "Episode: 360, running reward: 173.28\n",
      "Episode: 370, running reward: 152.99\n",
      "Episode: 380, running reward: 145.57\n",
      "Episode: 390, running reward: 152.03\n",
      "Episode: 400, running reward: 139.84\n",
      "Episode: 410, running reward: 163.51\n",
      "Episode: 420, running reward: 159.34\n",
      "Episode: 430, running reward: 151.23\n",
      "Episode: 440, running reward: 155.06\n",
      "Episode: 450, running reward: 161.09\n",
      "Episode: 460, running reward: 169.89\n",
      "Episode: 470, running reward: 167.71\n",
      "Episode: 480, running reward: 180.54\n",
      "Episode: 490, running reward: 188.35\n",
      "Episode: 500, running reward: 189.89\n",
      "Episode: 510, running reward: 193.35\n",
      "Solved at episode 516!\n"
     ]
    }
   ],
   "source": [
    "# зададим discount-фактор для награды\n",
    "gamma = 0.995\n",
    "\n",
    "# как долго длится эпизод\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# минимальная машинная точность\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# выделим списки для хранения значений\n",
    "# вероятности действий, критика и награды\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    # сбрасываем состояние к изначальному и записываем его\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # проводим раунд игры, записывая работу нашей нейросети в GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # идём по раунду шагами\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            env.render()  # для графического отображения окна с игрой\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # зная состояние, модель должна нам предсказать действие \n",
    "            # и поправку от критика\n",
    "            action_probs, critic_value = model(state)\n",
    "            \n",
    "            # поправку критика сразу сохраним\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # выберем действие, исходя из предсказанных вероятностей\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            \n",
    "            # вероятность выбранного действия сохраняем в историю\n",
    "            # можно сразу и прологарифмировать, как того требует наш лосс\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # говорим среде выполнить выбранное действие\n",
    "            # и получаем от неё следующее состояние и награду\n",
    "            # если игра окончена, среда вернут done=True\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # записываем полученную награду\n",
    "            rewards_history.append(reward)\n",
    "            \n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # пересчитываем running_reward с затуханием\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # награда на каждом шаге игры (то, что нам и должен предсказать критик)\n",
    "        # определяется как суммарная награда от этого и всех последующих шагов (с затуханием)\n",
    "        # считаем их все в 1 проход и добавляем в returns\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # нормализуем их для повышения стабильности обучения\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # осталось посчитать значения функций потерь, чтобы затем осуществить шаг градиентного спуска\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # на каждом шаге считаем лосс для actor и critic\n",
    "            # у первого это просто награда (за вычетом поправки от критика), \n",
    "            # умноженная на логарифмическую вероятность действия\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # критик же оканчивается обычной регрессией:\n",
    "            # он должен уметь предсказывать ожидаемую награду на каждом шаге\n",
    "            critic_losses.append(\n",
    "                loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # теперь можем осуществить стандартный шаг градиентного спуска\n",
    "        # чтобы запустить его из одной точки, сложим все лоссы\n",
    "        # т.к. при суммировании градиент от каждого слагаемого отправится\n",
    "        # только в направлении данного слагаемого, это то, что нужно\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # перед следующим эпизодом очистим все списки\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    episode_count += 1\n",
    "    \n",
    "    # каждые 10 эпизодов будем выводить running reward\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode: {episode_count}, running reward: {running_reward:.2f}\")\n",
    "\n",
    "    # если running reward стал больше 195, цель достигнута\n",
    "    if running_reward > 195:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n",
    "\n",
    "# саму среду в конце нужно закрыть\n",
    "# чтобы рендер прекратился и можно было дальше работать в питоне\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "state = tf.convert_to_tensor(state)\n",
    "state = tf.expand_dims(state, 0)\n",
    "action_probs, _ = model(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.8301848 , 0.16981524]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
