{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим датасет с помощью Keras\n",
    "# нам необходимы только входные данные (без лейблов)\n",
    "\n",
    "(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# соединим и отнормируем данные\n",
    "\n",
    "all_digits = np.concatenate([x_train, x_test])\n",
    "all_digits = all_digits.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# представим картинки в виде, соответствующем\n",
    "# свёрточному слою 28х28 пикселей с 1 каналом\n",
    "\n",
    "all_digits = np.reshape(all_digits, (-1, 28, 28, 1))\n",
    "\n",
    "# создадим датасет\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(all_digits)\n",
    "\n",
    "# картинки в датасете будут перемешиваться пачками по 1024\n",
    "# и подаваться батчами по 64, при этом во время просчёта очередного батча\n",
    "# следующие 32 картинки будут уже параллельно подготавливаться для ускорения работы\n",
    "\n",
    "batch_size = 64\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим дискриминатор - свёрточную нейросеть для бинарной классификации картинок\n",
    "\n",
    "discriminator = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(28, 28, 1)),\n",
    "        \n",
    "        # делаем последовательность свёрточных и pooling слоёв\n",
    "        # параметр padding=\"same\" задаёт паддинг таким образом, \n",
    "        # чтобы размер выходного слоя совпадал с предыдущим\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\"),\n",
    "        \n",
    "        # с Leaky ReLU наш GAN будет обучаться эффективнее\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        # ещё один такой же блок\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), strides=(1, 1), padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        # в конце выбираем максимальное значение среди всех в каждом канале\n",
    "        tf.keras.layers.GlobalMaxPooling2D(),\n",
    "        \n",
    "        # и ставим полносвязный слой из 1 нейрона для классификации\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ],\n",
    "    \n",
    "    name=\"discriminator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пусть эмбеддинг нашей картинки имеет размерность 128\n",
    "\n",
    "latent_dim = 128\n",
    "\n",
    "# создадим генератор\n",
    "\n",
    "generator = tf.keras.Sequential(\n",
    "    [\n",
    "        # на вход получаем вектор из latent space\n",
    "        tf.keras.Input(shape=(latent_dim,)),\n",
    "        \n",
    "        # прогоним его через Dense-слой, который потом можно будет развернуть в свёрточный\n",
    "        tf.keras.layers.Dense(14 * 14 * 128),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        # переформируем слой в свёрточный 7х7 со 128 каналами\n",
    "        tf.keras.layers.Reshape((14, 14, 128)),\n",
    "        \n",
    "        # используем \"развёрточные\" слои: они работают так же, \n",
    "        # как свёрточные, только в \"обратную сторону\"\n",
    "        tf.keras.layers.Conv2DTranspose(64, (3, 3), strides=(1, 1), padding=\"same\"),\n",
    "        tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "        \n",
    "        # апсемплинг - пулинг наоборот\n",
    "        # вместо одного пикселя генерируются 4 пикселя с таким значением\n",
    "        tf.keras.layers.UpSampling2D(size=(2, 2)),\n",
    "        \n",
    "\n",
    "        # в конце генерируем картинку с одним каналом\n",
    "        tf.keras.layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    \n",
    "    name=\"generator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 25088)             3236352   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        73792     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 1)         3137      \n",
      "=================================================================\n",
      "Total params: 3,313,281\n",
      "Trainable params: 3,313,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 74,625\n",
      "Trainable params: 74,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс должен наследоваться от базового tf.keras.Model\n",
    "\n",
    "class GAN(tf.keras.Model):\n",
    "    # в конструкторе принимаем генератор и дискриминатор, указывая latent_dim\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        # для правильной работы модели необходимо вызвать конструктор родительского класса\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    # переопределим метод compile, чтобы была возможность задавать разные оптимизаторы\n",
    "    # для обучения генератора и дискриминатора\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    # теперь главное: обучение\n",
    "    # в методе train_step описывается, что происходит на каждом шаге обучения\n",
    "    # нам на каждом шаге нужно сначала прогнать нагенерированные + реальные картинки\n",
    "    # и обучить дискриминатор, а потом прогнать только нагенерированные и обучить генератор\n",
    "    def train_step(self, real_images):\n",
    "        # первое измерение - батч, оно нам не нужно\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        \n",
    "        # размер батча будет определяться тем, сколько нам поступило настоящих картинок\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        \n",
    "        # генерируем такой же батч случайных нормально распределённых векторов из latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # прогоняем их через генератор, чтобы получить поддельные картинки\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # в дискриминатор будем подавать их вместе с реальными (с соответствующими лейблами)\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # фейковым изображениям будет соответствовать 1, а настоящим - 0\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        \n",
    "        # важный трюк при обучении GAN-ов - добавление случайного шума в лейблы\n",
    "        # это позволит немного удержать дискриминатор от переобучения\n",
    "        # уровень шума - это, конечно, тоже гиперпараметр\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # после того, как данные подготовлены, \n",
    "        # нужно сделать тренировочный шаг дискриминатора\n",
    "        # для этого прогоняем их через дискриминатор и функцию потерь,\n",
    "        # при этом нужно записать результаты вычислений в \"ленту\", объект GradientTape\n",
    "        # после прямого прогона эту ленту можно будет как бы \"размотать\" в обратном порядке,\n",
    "        # тем самым посчитав градиенты методом обратного распространения ошибки\n",
    "        with tf.GradientTape() as tape:\n",
    "            # получаем предсказания дискриминатора\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            # и кладём их в функцию потерь, сравнивая с настоящими лейблами\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        \n",
    "        # теперь, используя нашу ленту, считаем градиенты от loss-функции до всех весов\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        \n",
    "        # после того, как градиенты посчитаны, оптимизатор делает шаг в соовтетствующем направлении для всех весов\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # теперь пришло время обучить генератор\n",
    "        # для этого снова сгенерируем батч векторов из latent_space, \n",
    "        # прогоним его через нашу сеть и сделаем шаг обучения генератора\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # наша цель - обмануть дискриминатор, поэтому помечаем\n",
    "        # все картинки как \"настоящие\" (т.е. нулями)\n",
    "        # если дискриминатор выдаст ноль (обманется), это хорошо, и loss Будет маленький\n",
    "        # если же дискриминатор распознает фейк, loss будет большой, и это значит, нужно\n",
    "        # сильнее обновить веса для данного примера\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # аналогичным образом проводим вычисления, записывая их в \"ленту\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            # прогоняем наши вектора и через генератор, и через дискриминатор\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            # ответы подаём в loss-функцию\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "            \n",
    "        # теперь вычисляем градиенты по весам генератора (дискриминатор обновлять не нужно)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        \n",
    "        # делаем шаг оптимизатора по вычисленным градиентам (обновляем только веса генератора)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        \n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(\n",
    "    d_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим папку images\n",
    "\n",
    "import os\n",
    "\n",
    "os.mkdir('images1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# наследуем класс от базгового класса Callbak\n",
    "class GANMonitor(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=5, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    # метод с таким названием Tensorflow будет вызывать в конце каждой эпохи\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # сгенерируем несколько случайных векторов\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        \n",
    "        # и прогоним их через генератор, чтобы получить картинки\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        \n",
    "        # отнормируем картинки и представим в numpy\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        \n",
    "        # каждую картинку сохраним под определённым именем\n",
    "        for i in range(self.num_img):\n",
    "            img = tf.keras.preprocessing.image.array_to_img(generated_images[i])\n",
    "            img.save(f\"images1/{epoch}_img_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1094/1094 [==============================] - 1468s 1s/step - d_loss: 0.5811 - g_loss: 0.9601\n",
      "Epoch 2/30\n",
      "1069/1094 [============================>.] - ETA: 34s - d_loss: 0.5661 - g_loss: 1.0150"
     ]
    }
   ],
   "source": [
    "# коллбэк для сохранения промежуточных картинок\n",
    "img_monitoring = GANMonitor(num_img=5, latent_dim=latent_dim)\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# метод fit будет делать шаги в соответствии с тем, \n",
    "# как мы описали их в методе train_step\n",
    "gan.fit(dataset,  # в качестве обучающих данных передаём наш объект датасета, созданный ранее, он уже будет работать батчами\n",
    "        epochs=epochs, \n",
    "        callbacks=[img_monitoring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a5aeb36e0ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gan_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2220\u001b[0m           '`load_weights` requires h5py when loading weights from HDF5.')\n\u001b[1;32m   2221\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m       raise ValueError(\n\u001b[0m\u001b[1;32m   2223\u001b[0m           \u001b[0;34m'Unable to load weights saved in HDF5 format into a subclassed '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m           \u001b[0;34m'Model which has not created its variables yet. Call the Model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights."
     ]
    }
   ],
   "source": [
    "gan.load_weights('gan_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
