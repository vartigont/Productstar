{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df_pikabu_spam_posts.pd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов: 59073\n"
     ]
    }
   ],
   "source": [
    "words_set = set()\n",
    "\n",
    "for row in df.itertuples():\n",
    "    for word in row.title:\n",
    "        words_set.add(word)\n",
    "    for word in row.text:\n",
    "        words_set.add(word)\n",
    "\n",
    "print(f\"Всего слов: {len(words_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# используем словарь для хранения кол-ва вхождений каждого слова\n",
    "words_counter = {w: 0 for w in words_set}\n",
    "\n",
    "for row in df.itertuples():\n",
    "    for word in row.title:\n",
    "        words_counter[word] += 1\n",
    "    for word in row.text:\n",
    "        words_counter[word] += 1\n",
    "\n",
    "# преобразуем словарь в список и отсортируем его\n",
    "words_list = list(words_counter.items())\n",
    "words_list.sort(key=(lambda x: x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оставим только 5к слов\n",
    "words_list = words_list[:5000]\n",
    "\n",
    "# кол-во вхождений нам уже не нужно\n",
    "words_list = [k[0] for k in words_list]\n",
    "\n",
    "# для быстрого создания OHE полезно будет заранее пронумеровать каждое слово\n",
    "# чтобы не ждать выполнения операции получения позиции в списке\n",
    "words_ohe_positions = {words_list[i]: i for i in range(len(words_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# списки под заголовки и тексты, которые тоже будут закодированы списками\n",
    "titles = []\n",
    "texts = []\n",
    "\n",
    "# перебираем все строки\n",
    "for row in df.itertuples():\n",
    "    # сначала создаём шаблок с одними нулями\n",
    "    title_ohe = [0] * len(words_list)\n",
    "    for word in row.title:\n",
    "        try:\n",
    "            # если слово из заголовка присутствует в нашем словаре, увеличиваем счётчик на соответствующем месте\n",
    "            title_ohe[words_ohe_positions[word]] += 1\n",
    "        except:\n",
    "            # если слово отсутствует, словарь выкинет исключение - в таком случае просто продолжаем цикл\n",
    "            continue\n",
    "    # делаем то же самое и для текста поста\n",
    "    text_ohe = [0] * len(words_list)\n",
    "    for word in row.text:\n",
    "        try:\n",
    "            text_ohe[words_ohe_positions[word]] += 1\n",
    "        except:\n",
    "            continue\n",
    "    # добавляем получившуюся кодировку в списки заголовков и текстов\n",
    "    titles.append(title_ohe)\n",
    "    texts.append(text_ohe)\n",
    "\n",
    "# для работы с Keras информацию лучше держать в Numpy\n",
    "titles = np.array(titles)\n",
    "texts = np.array(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(df['bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# чтобы не заниматься копипастом 3 раза, сделаем функцию,\n",
    "# которая разделяет массив в нужной пропорции\n",
    "def train_val_test_split(x, val_frac=0.15, test_frac=0.15):\n",
    "    x_train = x[:round((1 - val_frac - test_frac) * len(x))]\n",
    "    x_val = x[round((1 - val_frac - test_frac) * len(x)):round((1 - test_frac) * len(x))]\n",
    "    x_test = x[round((1 - test_frac) * len(x)):]\n",
    "    return x_train, x_val, x_test\n",
    "\n",
    "\n",
    "titles_train, titles_val, titles_test = train_val_test_split(titles)\n",
    "texts_train, texts_val, texts_test = train_val_test_split(texts)\n",
    "y_train, y_val, y_test = train_val_test_split(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_input = tf.keras.layers.Input(shape=(5000,))\n",
    "text_input = tf.keras.layers.Input(shape=(5000,))\n",
    "\n",
    "title_dense_1 = tf.keras.layers.Dense(500, activation='relu')(title_input)\n",
    "text_dense_1 = tf.keras.layers.Dense(500, activation='relu')(text_input)\n",
    "\n",
    "title_bn_1 = tf.keras.layers.BatchNormalization()(title_dense_1)\n",
    "text_bn_1 = tf.keras.layers.BatchNormalization()(text_dense_1)\n",
    "\n",
    "text_dense_2 = tf.keras.layers.Dense(500, activation='relu')(text_bn_1)\n",
    "\n",
    "text_bn_2 = tf.keras.layers.BatchNormalization()(text_dense_2)\n",
    "\n",
    "add = tf.keras.layers.Add()([title_bn_1, text_bn_2])\n",
    "\n",
    "main_dense_1 = tf.keras.layers.Dense(300, activation='relu')(add)\n",
    "main_bn_1 = tf.keras.layers.BatchNormalization()(main_dense_1)\n",
    "drp1 = tf.keras.layers.Dropout(0.8)(main_bn_1)\n",
    "main_dense_2 = tf.keras.layers.Dense(100, activation='relu')(drp1)\n",
    "main_bn_2 = tf.keras.layers.BatchNormalization()(main_dense_2)\n",
    "drp2 = tf.keras.layers.Dropout(0.8)(main_bn_2)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(drp2)\n",
    "\n",
    "model = tf.keras.Model(inputs=[title_input, text_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 500)          2500500     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 5000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 500)          2000        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 500)          2500500     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 500)          250500      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 500)          2000        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 500)          2000        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 500)          0           batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 300)          150300      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 300)          1200        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 300)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 100)          30100       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 100)          400         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 100)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 1)            101         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,439,601\n",
      "Trainable params: 5,435,801\n",
      "Non-trainable params: 3,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.keras.metrics.binary_accuracy\n",
    "precision = tf.keras.metrics.Precision()\n",
    "recall = tf.keras.metrics.Recall()\n",
    "auc = tf.keras.metrics.AUC()\n",
    "\n",
    "def f1_metrics(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    return 2 * ((prec * rec) / (prec + rec + 1e-7))\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.binary_crossentropy,\n",
    "              metrics=[accuracy,\n",
    "                       precision,\n",
    "                       recall,\n",
    "                       f1_metrics,\n",
    "                       auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/first', \n",
    "                                             histogram_freq=1) # данный параметр нужен, чтобы мониторить гистограммы весов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "21/21 [==============================] - 9s 212ms/step - loss: 1.2646 - binary_accuracy: 0.5176 - precision: 0.2691 - recall: 0.5600 - f1_metrics: 0.3629 - auc: 0.5420 - val_loss: 0.5844 - val_binary_accuracy: 0.7690 - val_precision: 0.5556 - val_recall: 0.1136 - val_f1_metrics: 0.1824 - val_auc: 0.6664\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 2s 80ms/step - loss: 0.9088 - binary_accuracy: 0.5951 - precision: 0.3226 - recall: 0.6833 - f1_metrics: 0.4289 - auc: 0.6840 - val_loss: 0.5289 - val_binary_accuracy: 0.8317 - val_precision: 0.9634 - val_recall: 0.2992 - val_f1_metrics: 0.4528 - val_auc: 0.8293\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 2s 81ms/step - loss: 0.5812 - binary_accuracy: 0.7424 - precision: 0.4747 - recall: 0.8066 - f1_metrics: 0.5935 - auc: 0.8383 - val_loss: 0.4422 - val_binary_accuracy: 0.8881 - val_precision: 0.9728 - val_recall: 0.5417 - val_f1_metrics: 0.6812 - val_auc: 0.9128\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 2s 80ms/step - loss: 0.3670 - binary_accuracy: 0.8472 - precision: 0.6385 - recall: 0.8679 - f1_metrics: 0.7287 - auc: 0.9315 - val_loss: 0.3450 - val_binary_accuracy: 0.9024 - val_precision: 0.9586 - val_recall: 0.6136 - val_f1_metrics: 0.7489 - val_auc: 0.9435\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 2s 80ms/step - loss: 0.2233 - binary_accuracy: 0.9216 - precision: 0.7867 - recall: 0.9300 - f1_metrics: 0.8511 - auc: 0.9744 - val_loss: 0.2864 - val_binary_accuracy: 0.9078 - val_precision: 0.9600 - val_recall: 0.6364 - val_f1_metrics: 0.7631 - val_auc: 0.9498\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 2s 81ms/step - loss: 0.1670 - binary_accuracy: 0.9463 - precision: 0.8437 - recall: 0.9474 - f1_metrics: 0.8752 - auc: 0.9838 - val_loss: 0.2557 - val_binary_accuracy: 0.8988 - val_precision: 0.9689 - val_recall: 0.5909 - val_f1_metrics: 0.7323 - val_auc: 0.9664\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 2s 83ms/step - loss: 0.1092 - binary_accuracy: 0.9758 - precision: 0.9300 - recall: 0.9728 - f1_metrics: 0.9519 - auc: 0.9917 - val_loss: 0.2333 - val_binary_accuracy: 0.9033 - val_precision: 0.9643 - val_recall: 0.6136 - val_f1_metrics: 0.7500 - val_auc: 0.9711\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 2s 75ms/step - loss: 0.0770 - binary_accuracy: 0.9862 - precision: 0.9656 - recall: 0.9771 - f1_metrics: 0.9710 - auc: 0.9937 - val_loss: 0.2180 - val_binary_accuracy: 0.9105 - val_precision: 0.9659 - val_recall: 0.6439 - val_f1_metrics: 0.7750 - val_auc: 0.9725\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 2s 76ms/step - loss: 0.0555 - binary_accuracy: 0.9899 - precision: 0.9709 - recall: 0.9854 - f1_metrics: 0.9830 - auc: 0.9984 - val_loss: 0.2159 - val_binary_accuracy: 0.9114 - val_precision: 0.9661 - val_recall: 0.6477 - val_f1_metrics: 0.7780 - val_auc: 0.9710\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 2s 87ms/step - loss: 0.0465 - binary_accuracy: 0.9936 - precision: 0.9792 - recall: 0.9934 - f1_metrics: 0.9854 - auc: 0.9992 - val_loss: 0.2104 - val_binary_accuracy: 0.9123 - val_precision: 0.9611 - val_recall: 0.6553 - val_f1_metrics: 0.7922 - val_auc: 0.9756\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21f92664d00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([titles_train, texts_train], y_train,  # данные на вход указываем в списке в нужном порядке\n",
    "          validation_data=([titles_val, texts_val], y_val),\n",
    "          batch_size=256,\n",
    "          epochs=10,\n",
    "          callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
